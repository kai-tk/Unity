Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
1000,3.6664758,28.2,-28.05356,-25730.147058823528,-25730.147058823528,153883680.0,0.24443087,0.00029969416,0.19989806,0.0004995004,1.0
2000,3.5832605,29.8125,-726.7827,-26882.4375,-26882.4375,163870270.0,0.25989673,0.00029911834,0.19970612,0.00049855997,1.0
3000,3.3914042,19.387755102040817,-1688.4144,-18694.836734693876,-18694.836734693876,84595384.0,0.25000873,0.00029850792,0.19950265,0.0004975629,1.0
4000,3.0486882,15.666666666666666,-2892.7424,-15255.1,-15255.1,42467910.0,0.23482293,0.0002979151,0.19930504,0.00049659476,1.0
5000,2.71271,14.461538461538462,-3540.5703,-14118.646153846154,-14118.646153846154,26077816.0,0.25540182,0.00029729848,0.1990995,0.0004955875,1.0
6000,2.2820451,12.63013698630137,-4173.009,-12475.849315068494,-12475.849315068494,14080534.0,0.2603098,0.00029668727,0.19889578,0.0004945892,1.0
