Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
1000,3.6248682,28.794117647058822,-0.0033288097,-0.03706644396438743,-0.03706644396438743,0.00060522533,0.22211652,0.0002996775,0.1998925,0.0004994733,1.0
2000,3.4427416,29.545454545454547,-0.010762825,-0.03643302320305145,-0.03643302320305145,0.0001985235,0.24547155,0.0002990746,0.19969153,0.0004984885,1.0
3000,3.457619,27.34285714285714,-0.019961359,-0.03542307104383196,-0.03542307104383196,0.00033164342,0.24095222,0.00029848344,0.19949448,0.00049752294,1.0
4000,3.1928356,22.441860465116278,-0.010235548,-0.03319683619016825,-0.03319683619016825,0.0003334639,0.23718312,0.00029790742,0.19930246,0.00049658207,1.0
5000,2.3404083,20.02127659574468,-0.015673624,-0.03240702491491399,-0.03240702491491399,0.00014851535,0.23480147,0.0002973132,0.19910441,0.0004956116,1.0
6000,2.1360486,19.75,-0.022065787,-0.031082600083512563,-0.031082600083512563,0.0030234447,0.24990895,0.00029671847,0.19890615,0.00049464015,1.0
7000,2.357667,20.170212765957448,-0.018154606,-0.03062568224490957,-0.03062568224490957,0.00022085513,0.24146472,0.00029609722,0.19869907,0.0004936254,1.0
8000,1.6405557,17.943396226415093,-0.016138958,-0.02970459073219659,-0.02970459073219659,8.4541185e-05,0.24587218,0.00029548816,0.19849604,0.0004926307,1.0
