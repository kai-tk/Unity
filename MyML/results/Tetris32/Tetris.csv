Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
1000,3.596676,31.125,4.565831,49.957676103038175,49.957676103038175,2730.5525,0.24526663,0.0002996825,0.19989416,0.00049948134,1.0
2000,3.5316582,28.818181818181817,10.989757,44.64024084987062,44.64024084987062,1807.8494,0.2330591,0.0002991175,0.19970584,0.00049855857,1.0
3000,3.4689922,28.08823529411765,14.631465,44.68635207064011,44.68635207064011,1797.6614,0.2516898,0.0002985207,0.19950688,0.00049758377,1.0
4000,3.461072,28.764705882352942,20.891958,42.899057037690106,42.899057037690106,1376.3579,0.25972736,0.00029790873,0.19930293,0.0004965843,1.0
5000,3.3673046,28.764705882352942,24.69239,35.55341009532704,35.55341009532704,1028.174,0.25008366,0.00029730517,0.19910172,0.0004955984,1.0
