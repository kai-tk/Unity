Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
1000,3.6677039,31.419354838709676,2.2453575,102.58533147176107,102.58533147176107,4562.4526,0.2530574,0.00029969527,0.1998984,0.0004995023,1.0
2000,3.6334639,30.28125,7.3793907,106.18249726295471,106.18249726295471,2892.2898,0.25722829,0.00029910242,0.1997008,0.00049853395,1.0
3000,3.5916781,31.193548387096776,11.31017,99.04773964420441,99.04773964420441,2082.1174,0.2645638,0.00029850152,0.19950049,0.00049755245,1.0
4000,3.621913,31.774193548387096,16.86559,101.0129011830976,101.0129011830976,1762.252,0.23090416,0.00029790655,0.19930217,0.0004965807,1.0
5000,3.6395142,31.833333333333332,17.96842,94.57532946268718,94.57532946268718,2286.1294,0.23265612,0.0002972932,0.19909774,0.0004955788,1.0
6000,3.5205936,33.58620689655172,20.109097,126.29103200188999,126.29103200188999,4930.163,0.24178487,0.0002966871,0.1988957,0.00049458886,1.0
