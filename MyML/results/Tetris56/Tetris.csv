Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
1000,3.6316955,29.818181818181817,0.09399417,0.3596250154078007,0.3596250154078007,0.006410444,0.25220835,0.00029967283,0.19989094,0.0004994656,1.0
2000,3.3466465,29.03030303030303,0.06376559,0.33960608009136084,0.33960608009136084,0.0073549654,0.23091316,0.00029907833,0.19969279,0.0004984946,1.0
3000,3.2978349,30.96875,0.062027935,0.42918751761317253,0.42918751761317253,0.004806547,0.22611615,0.00029848077,0.19949359,0.0004975186,1.0
4000,3.2144766,29.5625,0.08807646,0.38090627058409154,0.38090627058409154,0.0058201677,0.24456862,0.00029787252,0.19929084,0.0004965251,1.0
