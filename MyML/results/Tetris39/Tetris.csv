Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
1000,3.6176498,31.580645161290324,-9.325877,-10.91666612625122,-10.91666612625122,1191.5841,0.23314577,0.00029967807,0.19989268,0.0004994742,1.0
2000,3.4575093,29.058823529411764,-15.333351,-19.824705123901367,-19.824705123901367,340.02246,0.22806531,0.00029909622,0.19969875,0.0004985238,1.0
3000,3.462512,29.9375,-20.970936,-15.100937247276306,-15.100937247276306,143.99394,0.24598122,0.00029849567,0.19949856,0.0004975429,1.0
4000,3.3100035,31.6,-24.421316,-20.263665199279785,-20.263665199279785,188.03728,0.2574224,0.0002979249,0.1993083,0.0004966107,1.0
5000,3.191357,28.647058823529413,-24.165548,-15.279116125667796,-15.279116125667796,371.7389,0.25531903,0.00029730087,0.19910032,0.00049559143,1.0
6000,3.1335223,33.166666666666664,-24.930096,-11.061998112996418,-11.061998112996418,372.75122,0.255435,0.0002966915,0.19889715,0.000494596,1.0
