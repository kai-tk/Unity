Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
1000,3.640578,28.764705882352942,11.6961,51.952514792933606,51.952514792933606,1211.9934,0.25519204,0.00029966913,0.19988972,0.00049945957,1.0
2000,3.5743961,29.939393939393938,25.536232,52.25918186072147,52.25918186072147,622.5011,0.2511847,0.0002991091,0.19970304,0.0004985448,1.0
3000,3.5510335,32.1,36.420338,61.31773252487183,61.31773252487183,565.46625,0.25487328,0.00029850437,0.19950144,0.00049755705,1.0
4000,3.4640157,32.93103448275862,41.25446,59.768552122444945,59.768552122444945,518.69904,0.24855745,0.00029789342,0.19929779,0.0004965592,1.0
5000,3.2993066,29.696969696969695,44.288006,60.316635709820375,60.316635709820375,471.5618,0.25188482,0.0002973177,0.19910589,0.00049561885,1.0
