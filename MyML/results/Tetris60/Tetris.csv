Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
1000,3.57994,29.323529411764707,-0.32519168,-0.09398189999840477,-0.09398189999840477,0.22941709,0.25332624,0.00029967542,0.1998918,0.00049946987,1.0
2000,3.4911928,27.705882352941178,-0.55402565,-0.27480888980276447,-0.27480888980276447,0.061343357,0.27401435,0.0002990986,0.19969955,0.0004985277,1.0
3000,3.3775854,32.12903225806452,-0.43503916,-0.04772911148686563,-0.04772911148686563,0.12833771,0.24135225,0.00029851982,0.19950658,0.00049758237,1.0
4000,3.4369571,29.59375,-0.36365622,-0.029203172773122787,-0.029203172773122787,0.13595474,0.24770385,0.00029788155,0.19929385,0.00049653987,1.0
5000,3.2706263,29.84375,-0.42276365,-0.03545946441590786,-0.03545946441590786,0.15647812,0.26777726,0.00029728818,0.19909605,0.0004955706,1.0
6000,3.2889302,30.78125,-0.45997667,-0.08538758382201195,-0.08538758382201195,0.12037553,0.23613341,0.0002967046,0.19890153,0.0004946176,1.0
7000,3.3989692,31.387096774193548,-0.49800983,-0.1274581282369552,-0.1274581282369552,0.10182779,0.23893869,0.00029611262,0.19870418,0.0004936506,1.0
8000,3.0446155,29.606060606060606,-0.5256396,-0.22930613069823294,-0.22930613069823294,0.053067647,0.22538736,0.000295515,0.19850501,0.0004926744,1.0
9000,2.913517,29.65625,-0.48547786,-0.04379694443196058,-0.04379694443196058,0.1299895,0.24793631,0.0002949203,0.19830674,0.00049170316,1.0
