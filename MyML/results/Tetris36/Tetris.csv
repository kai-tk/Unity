Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
1000,3.5687852,30.0,10.18917,45.610625721514225,45.610625721514225,1077.6884,0.23791817,0.0002996715,0.1998905,0.0004994634,1.0
2000,3.48041,32.93103448275862,31.012903,49.17069073381095,49.17069073381095,425.2235,0.26194134,0.0002990972,0.19969907,0.00049852545,1.0
3000,3.465163,27.65714285714286,26.597599,39.67085811070034,39.67085811070034,503.99527,0.24586174,0.0002984968,0.19949894,0.0004975448,1.0
4000,3.414269,32.166666666666664,34.314373,44.772334575653076,44.772334575653076,453.96582,0.2389605,0.00029790198,0.19930065,0.00049657316,1.0
5000,3.2450762,32.56666666666667,41.135155,52.475334866841635,52.475334866841635,378.6276,0.23366587,0.0002973132,0.19910441,0.0004956115,1.0
6000,3.1601105,32.16129032258065,44.42518,55.33129058345671,55.33129058345671,419.26663,0.24164176,0.00029667694,0.1988923,0.00049457233,1.0
7000,3.0024643,34.666666666666664,45.995018,57.401481275205256,57.401481275205256,215.7431,0.23532745,0.00029608043,0.19869347,0.00049359805,1.0
8000,2.9710042,34.06896551724138,46.073254,61.49344884938207,61.49344884938207,230.02982,0.24677348,0.00029549867,0.19849956,0.0004926478,1.0
