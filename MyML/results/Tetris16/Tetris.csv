Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
1000,3.6393387,28.441176470588236,-8.152335,-119.0016457528779,-119.0016457528779,6427.0884,0.25303254,0.00029967434,0.19989145,0.0004994681,1.0
2000,3.3064222,19.979166666666668,-19.169683,-66.50779056549072,-66.50779056549072,1770.5973,0.24035488,0.0002990918,0.19969726,0.00049851666,1.0
3000,2.8560135,16.596491228070175,-22.215263,-49.00399713348924,-49.00399713348924,807.9192,0.26847285,0.00029848254,0.19949417,0.0004975215,1.0
4000,2.3598964,14.257575757575758,-24.427391,-37.784749031066895,-37.784749031066895,369.09924,0.24761364,0.00029787963,0.19929321,0.0004965367,1.0
5000,1.907989,13.544117647058824,-24.819447,-36.52631218293134,-36.52631218293134,352.26453,0.24623807,0.00029728582,0.19909528,0.00049556675,1.0
6000,1.4174267,12.726027397260275,-25.491789,-37.169862694936256,-37.169862694936256,362.32712,0.26165268,0.00029670834,0.19890276,0.0004946237,1.0
