Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
1000,3.6075172,31.903225806451612,-0.72963357,-1.1172460258007049,-1.1172460258007049,0.35407948,0.24144208,0.00029969536,0.19989844,0.0004995024,1.0
2000,3.5093403,30.3125,-0.93830854,-0.8686075895093381,-0.8686075895093381,2.1200948,0.23428352,0.00029910885,0.19970296,0.00049854454,1.0
3000,3.5117886,31.032258064516128,-0.9811816,-1.054813575360083,-1.054813575360083,0.42774528,0.23466246,0.0002985078,0.19950262,0.0004975628,1.0
4000,3.3412752,33.06896551724138,-0.8874805,-1.1773569301284592,-1.1773569301284592,0.20474999,0.25383347,0.0002979361,0.19931203,0.0004966289,1.0
5000,3.3708107,30.25,-1.0499678,-1.2277881354093552,-1.2277881354093552,0.12653129,0.23607448,0.00029732333,0.19910778,0.00049562816,1.0
6000,3.1296089,30.3125,-1.0355608,-1.2389584686607122,-1.2389584686607122,0.3624136,0.2680665,0.00029670328,0.19890109,0.00049461535,1.0
7000,3.0288925,29.545454545454547,-1.0282001,-1.1580660722472451,-1.1580660722472451,0.14898887,0.2466059,0.00029610604,0.19870201,0.0004936398,1.0
