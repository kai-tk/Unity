Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
1000,3.6116323,30.484848484848484,-9.094061,-21.499998211860657,-21.499998211860657,626.87964,0.23490936,0.00029967818,0.19989273,0.00049947435,1.0
2000,3.4905522,31.433333333333334,-19.588158,-16.66499964396159,-16.66499964396159,229.04422,0.23690909,0.00029910437,0.19970144,0.0004985371,1.0
3000,3.3598542,31.774193548387096,-22.82185,-15.372579882221837,-15.372579882221837,168.62152,0.22727102,0.00029850908,0.19950302,0.00049756485,1.0
4000,3.3362193,34.0,-23.46828,-7.293569019862583,-7.293569019862583,630.6845,0.23561184,0.00029788254,0.19929416,0.0004965415,1.0
5000,3.201596,33.0,-18.172716,-6.969332949320475,-6.969332949320475,1209.9429,0.26128668,0.00029730136,0.19910045,0.0004955922,1.0
6000,3.142994,34.857142857142854,-22.775911,-10.049283913203649,-10.049283913203649,254.80524,0.23656994,0.00029669187,0.1988973,0.0004945967,1.0
7000,3.242457,32.758620689655174,-18.953527,-9.957585170351226,-9.957585170351226,367.1548,0.24518213,0.00029610068,0.19870022,0.0004936311,1.0
8000,3.3070366,31.70967741935484,-17.60973,-14.635161615187123,-14.635161615187123,461.273,0.26313114,0.0002955055,0.19850184,0.00049265905,1.0
