Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
1000,3.5963228,29.5,-2.1847196,-31.93908836624839,-31.93908836624839,2087.2693,0.25499666,0.00029969183,0.19989727,0.0004994967,1.0
2000,3.4875474,29.96875,-10.672549,-14.259957730770111,-14.259957730770111,2490.7747,0.25727075,0.00029912026,0.19970673,0.0004985631,1.0
3000,3.4232726,29.8125,-15.20168,-23.238121554255486,-23.238121554255486,1311.1815,0.24165566,0.00029849348,0.19949783,0.0004975393,1.0
4000,3.342016,26.47222222222222,-19.165956,-29.148053930865395,-29.148053930865395,1059.0536,0.25105363,0.0002979123,0.1993041,0.00049659016,1.0
5000,2.8384638,19.18,-19.632425,-7.3027988529205325,-7.3027988529205325,974.8086,0.23864347,0.00029732613,0.19910869,0.0004956326,1.0
6000,2.1090882,15.40983606557377,-18.773005,3.983115170822769,3.983115170822769,355.8497,0.247928,0.00029670127,0.19890043,0.00049461203,1.0
