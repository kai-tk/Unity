Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
1000,3.6709483,27.97142857142857,-2.390889,-229.46999291812674,-229.46999291812674,12239.236,0.25011382,0.0002996967,0.1998989,0.0004995047,1.0
2000,3.5177305,22.5,-16.840517,-197.3361890883673,-197.3361890883673,7464.7007,0.23060091,0.00029910312,0.19970104,0.00049853505,1.0
3000,3.3179507,18.346153846153847,-30.876251,-165.83730653616098,-165.83730653616098,3821.0166,0.25539863,0.00029848784,0.19949593,0.00049753004,1.0
4000,2.978815,14.323076923076924,-35.203846,-134.96553743802585,-134.96553743802585,1871.6752,0.23706788,0.00029789947,0.19929983,0.00049656915,1.0
