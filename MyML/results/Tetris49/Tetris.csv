Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
1000,3.6253335,29.333333333333332,-0.76395833,-1.1718034781515598,-1.1718034781515598,1.6367532,0.26614273,0.00029967833,0.19989277,0.0004994746,1.0
2000,3.468207,29.848484848484848,-1.2822262,-1.6733664019089756,-1.6733664019089756,0.18505031,0.26214767,0.00029910123,0.19970042,0.00049853197,1.0
3000,3.4356613,30.40625,-1.3540373,-1.8724959809333086,-1.8724959809333086,0.04751299,0.25356585,0.00029848242,0.19949414,0.0004975212,1.0
4000,3.428623,26.305555555555557,-1.3884683,-1.803054200278388,-1.803054200278388,0.026324775,0.24205872,0.00029789266,0.19929756,0.00049655803,1.0
