Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
1000,3.6490695,28.852941176470587,-3.0482233,-11.092030062819973,-11.092030062819973,31.711918,0.25095987,0.00029970694,0.1999023,0.0004995213,1.0
2000,3.5414104,28.264705882352942,-6.501661,-11.408411502838135,-11.408411502838135,7.397265,0.24644367,0.0002991128,0.19970427,0.0004985508,1.0
3000,3.4254665,28.705882352941178,-6.988791,-11.750293829861809,-11.750293829861809,12.655638,0.25600564,0.00029847943,0.19949314,0.00049751636,1.0
4000,3.2886558,27.88235294117647,-7.6283073,-10.035617414642783,-10.035617414642783,6.9043527,0.23349133,0.00029790588,0.19930196,0.00049657957,1.0
5000,3.0908053,23.536585365853657,-5.766444,-8.34470727967053,-8.34470727967053,3.0217307,0.24752791,0.0002973242,0.19910806,0.00049562956,1.0
6000,2.8861346,18.0,-4.3358207,-5.682830059303428,-5.682830059303428,1.3928045,0.26061442,0.00029672755,0.19890918,0.000494655,1.0
7000,2.6247625,13.701492537313433,-3.5385954,-4.634298399313172,-4.634298399313172,0.74404526,0.23189595,0.00029612202,0.19870733,0.0004936659,1.0
8000,2.1447299,12.453333333333333,-3.2388868,-4.214586588541667,-4.214586588541667,0.29897162,0.23546831,0.0002955015,0.1985005,0.0004926523,1.0
9000,1.5632758,11.794871794871796,-3.024128,-4.185012756249844,-4.185012756249844,0.5963943,0.24819323,0.0002948873,0.19829576,0.0004916492,1.0
