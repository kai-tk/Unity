Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
1000,3.6324952,32.46666666666667,2.8682947,180.30772294669316,180.30772294669316,5843.009,0.27981198,0.00029970243,0.19990079,0.000499514,1.0
2000,3.5201497,30.34375,9.181169,170.0748529434204,170.0748529434204,4369.9507,0.24822259,0.000299116,0.19970532,0.0004985562,1.0
3000,3.491683,30.90625,13.206669,167.12942600250244,167.12942600250244,3940.5156,0.24805135,0.00029850064,0.1995002,0.00049755094,1.0
4000,3.3916075,30.64516129032258,14.304594,178.5595929545741,178.5595929545741,6813.917,0.23557177,0.00029790407,0.19930136,0.00049657654,1.0
5000,3.169787,30.125,19.387281,161.81980729103088,161.81980729103088,3623.5796,0.26307994,0.0002973226,0.19910753,0.00049562694,1.0
6000,2.9746659,30.28125,22.50922,156.9669578075409,156.9669578075409,2974.9065,0.2544657,0.00029669114,0.19889705,0.0004945955,1.0
7000,2.3806267,30.75,24.937449,166.4274435043335,166.4274435043335,3361.0137,0.24714407,0.00029609643,0.1986988,0.00049362413,1.0
