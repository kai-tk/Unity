Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
1000,3.6818624,31.806451612903224,-1.1468686,-174.51932474772136,-174.51932474772136,6391.817,0.235374,0.00029969122,0.19989708,0.0004994957,1.0
2000,3.6067278,27.485714285714284,-5.6277695,-156.12542288643974,-156.12542288643974,4082.3108,0.24395138,0.00029908636,0.19969545,0.0004985077,1.0
3000,3.5259182,27.65714285714286,-13.068812,-156.64085344587053,-156.64085344587053,3523.6055,0.2524878,0.00029849337,0.19949779,0.0004975391,1.0
4000,3.4232783,22.666666666666668,-20.849705,-124.52594629923503,-124.52594629923503,1754.1934,0.25021863,0.00029792948,0.19930983,0.00049661816,1.0
5000,3.0977755,15.983050847457626,-23.952984,-74.4869453300864,-74.4869453300864,1217.1226,0.22742128,0.00029729833,0.19909944,0.0004955873,1.0
6000,2.7783413,12.58904109589041,-13.7717085,-54.01602434132197,-54.01602434132197,507.59613,0.23963223,0.0002966863,0.19889542,0.0004945875,1.0
7000,2.4729276,11.884615384615385,-5.13442,-45.98833147684733,-45.98833147684733,314.28537,0.24559028,0.00029610895,0.19870298,0.0004936446,1.0
8000,1.8740944,11.683544303797468,-1.0185286,-44.24569311021249,-44.24569311021249,226.63956,0.23002943,0.00029550475,0.19850162,0.00049265777,1.0
9000,1.324499,11.209876543209877,2.0287158,-41.39641754715531,-41.39641754715531,180.20218,0.23141997,0.0002949029,0.19830097,0.0004916747,1.0
