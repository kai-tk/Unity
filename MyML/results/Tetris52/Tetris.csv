Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
1000,3.6436887,28.88235294117647,-0.18743855,-0.24788515947081827,-0.24788515947081827,0.12024503,0.25046688,0.00029966934,0.19988978,0.0004994599,1.0
2000,3.3630471,19.708333333333332,-0.21435522,-0.27837688631067675,-0.27837688631067675,0.0013564889,0.23122269,0.0002990716,0.19969054,0.0004984836,1.0
3000,2.9273312,20.170212765957448,-0.20527917,-0.23740724077884187,-0.23740724077884187,0.024676312,0.24359614,0.00029848385,0.19949462,0.0004975235,1.0
4000,2.9925485,20.170212765957448,-0.19259605,-0.24237618262463428,-0.24237618262463428,0.017607884,0.2544293,0.00029789153,0.19929719,0.0004965562,1.0
5000,2.7782423,16.875,-0.1668479,-0.17566679710788385,-0.17566679710788385,0.032377407,0.2275664,0.00029730637,0.1991021,0.0004956004,1.0
