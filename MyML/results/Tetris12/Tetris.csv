Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
1000,3.6466296,31.258064516129032,0.048581775,0.053396095211307205,0.053396095211307205,0.0019929998,0.24425133,0.0002996681,0.19988938,0.00049945794,1.0
2000,3.5107176,28.852941176470587,0.017710866,0.049142192391788256,0.049142192391788256,0.00023457721,0.24417648,0.00029907978,0.19969325,0.0004984969,1.0
3000,3.409557,30.15625,0.019770797,0.04765463824151084,0.04765463824151084,0.00048400252,0.2253437,0.00029850224,0.19950077,0.0004975537,1.0
4000,3.1001463,30.78125,0.01264067,0.05101087602088228,0.05101087602088228,0.00042343023,0.24449492,0.00029791694,0.19930565,0.00049659767,1.0
5000,2.9700036,28.558823529411764,0.020782446,0.0496686380904387,0.0496686380904387,0.00025909592,0.22532552,0.00029729612,0.19909872,0.00049558375,1.0
6000,2.9565353,32.48275862068966,0.023076043,0.056672634813806105,0.056672634813806105,0.0005449193,0.23967358,0.0002967175,0.19890584,0.0004946385,1.0
7000,2.9220207,31.838709677419356,0.025686601,0.05823020312574602,0.05823020312574602,0.0004093204,0.23523548,0.000296104,0.19870132,0.00049363653,1.0
