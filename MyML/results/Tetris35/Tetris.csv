Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
1000,3.6079626,29.147058823529413,8.253721,45.22757623773633,45.22757623773633,1103.6238,0.23632017,0.00029968863,0.1998962,0.00049949135,1.0
2000,3.509387,32.1,26.41137,49.151333554585776,49.151333554585776,537.9571,0.23501447,0.00029909864,0.19969955,0.0004985279,1.0
3000,3.3469598,28.735294117647058,27.98503,35.57323635325712,35.57323635325712,525.96045,0.22650117,0.00029851246,0.19950415,0.0004975704,1.0
4000,3.3847725,30.93548387096774,32.76327,45.488388246105565,45.488388246105565,367.2836,0.2330107,0.00029790882,0.19930294,0.0004965844,1.0
5000,3.2643065,32.333333333333336,39.710964,44.44366801579793,44.44366801579793,465.44873,0.23605023,0.00029729467,0.19909823,0.00049558125,1.0
6000,3.1853056,33.166666666666664,39.631172,44.8173347791036,44.8173347791036,350.04236,0.2545942,0.00029669207,0.19889735,0.000494597,1.0
7000,3.2091396,32.0,42.357723,46.07433422406515,46.07433422406515,421.62598,0.24673836,0.0002960923,0.19869743,0.0004936174,1.0
8000,3.0704699,35.2962962962963,48.28403,63.39703782399496,63.39703782399496,321.55276,0.2400063,0.00029550053,0.19850019,0.0004926509,1.0
