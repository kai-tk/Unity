Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
1000,3.6212335,28.4,0.018011814,0.00738473457065137,0.00738473457065137,0.53811264,0.24997921,0.0002996736,0.19989121,0.0004994669,1.0
2000,3.5622618,27.514285714285716,7.409787e-05,0.0002908671894277047,0.0002908671894277047,6.637097e-05,0.24889547,0.0002990804,0.19969346,0.000498498,1.0
3000,3.481418,27.61764705882353,0.001431886,0.0002905779363251949,0.0002905779363251949,1.3006149e-05,0.2494646,0.00029851383,0.19950461,0.0004975726,1.0
4000,3.2112093,29.96969696969697,-0.014713001,0.00882650547877843,0.00882650547877843,9.171799e-06,0.25472328,0.00029793775,0.19931257,0.0004966317,1.0
5000,3.0659466,26.63888888888889,0.007007512,0.0002846653899016221,0.0002846653899016221,0.1871276,0.24806152,0.00029730328,0.19910109,0.00049559533,1.0
6000,2.7152112,22.209302325581394,-0.00052864134,0.00024213248371081643,0.00024213248371081643,7.1241634e-06,0.2598766,0.0002966646,0.1988882,0.0004945522,1.0
