Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
1000,3.6047652,30.9375,11.518047,54.94709753221081,54.94709753221081,915.0861,0.25338113,0.00029965967,0.19988653,0.00049944414,1.0
2000,3.4932642,30.25,36.239594,54.5221888422966,54.5221888422966,183.71788,0.2585222,0.0002990992,0.19969973,0.0004985287,1.0
3000,3.2358596,29.78125,39.726665,58.38562536239624,58.38562536239624,120.52037,0.21653959,0.00029849436,0.19949812,0.0004975407,1.0
4000,3.300032,25.38888888888889,42.780075,55.32500021081222,55.32500021081222,107.64939,0.23732927,0.00029775818,0.19925272,0.0004963383,1.0
5000,3.289353,25.31578947368421,42.895027,54.339737239636875,54.339737239636875,90.86774,0.24430424,0.00029729248,0.1990975,0.00049557775,1.0
6000,3.1951828,22.74418604651163,45.467705,53.02558215828829,53.02558215828829,134.26115,0.26457217,0.00029669388,0.19889796,0.0004946,1.0
7000,3.2927072,27.257142857142856,42.73859,57.14085758754185,57.14085758754185,92.16464,0.23376209,0.0002961103,0.19870345,0.00049364683,1.0
